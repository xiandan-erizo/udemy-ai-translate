<div class="transcript--transcript-panel--JLceZ" data-purpose="transcript-panel" dir="auto"><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue-active" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="transcript--highlight-cue--ugVsE">Next, let's talk about L1 and L2 regularization.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">This is more broad than just deep learning.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">This is something that applies to the entire field of machine learning.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">But the exam will expect you to know the difference between the two and when you might apply one over</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">the other.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">Again, this is sort of a more advanced thing, but you know, that's what this exam is all about.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">It's seeing how advanced you are in machine learning.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">So the way this works is that it adds what we call a regularization term.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">As your weights are being learned in your neural network or machine learning model.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">L1 is just the sum of those weights, and L2 is the sum of the square of those weights.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">So graphically on the right here you can see that L1 ends up being that sort of diamond shape, whereas</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">L2 being the square ends up being more round or.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">Well, it is round.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">It's a circle.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">So you can also apply the same idea to loss functions as well, and not just for regularization during</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">training.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">So what's the practical difference between these two things.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">Well L1 as the sum of weights.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">In practice it ends up performing feature selection.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">Election mathematically can cause entire features to go to zero, so that regularization term can end</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">up going to zero for a lot of terms, and actually choose some features that are more important than</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">others.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">It is, however, computationally inefficient and results in sparse output because it's removing information</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">at the end of the day.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">L2, however, the sum of the square of the weights.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">This causes all features to remain considered.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">It just weights them all differently.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">So nothing goes to zero.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">You just get very small or very large weights for different features based on how things shake out.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">It's more efficient computationally and results in denser output because it's not discarding anything.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">So L2 sounds pretty cool, right?</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">I mean, I mean, I want to keep my information, don't I?</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">But there are cases where you'd want L1.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">So again, we talked about the curse of dimensionality when we started talking about feature engineering</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">and feature selection with L1.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">Regularization is one way of doing that automatically.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">And, you know, in an extreme example, out of 100 different features you have, maybe only ten would</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">actually end up with non-zero coefficients with L1 regularization.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">So the resulting sparsity that you end up with can make up for the computational inefficiency of L1</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">regularization itself.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">So even though it's a little bit more <doubao-vocabulary-highlight data-key="intensive">intensive</doubao-vocabulary-highlight> to compute L1 regularization, you end up with a much</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">smaller set of features at the end of the day, which can speed up the training of your machine learning</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">model considerably.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">Right.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">So at the end of the day, it's probably a win as far as total training time goes.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">So if you think that you're in a world where some of your features might not matter and you actually</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">want to reduce that down to a smaller subset of features, L1 is probably going to work out well for</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">you.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">However, if you think all your features are important, then go with L2 regularization because that's</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">not going to do feature selection.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">It's not going to wipe out entire features by causing that regularization term to go all the way down</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">to zero.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">It will just weight them lower.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">So that's the difference.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">L1 does feature selection.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">L2 keeps them all around, but just weights them all differently.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">And well that's the main difference guys.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">That's all there is.</span></p></div><div class="transcript--cue-container--Vuwj6"><p data-purpose="transcript-cue" class="transcript--underline-cue---xybZ" role="button" tabindex="-1"><span data-purpose="cue-text" class="">L1 and L2 regularization.</span></p></div></div>